{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11645589,"sourceType":"datasetVersion","datasetId":7307714}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --no-cache-dir transformers datasets seqeval --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:14:33.313623Z","iopub.execute_input":"2025-05-02T07:14:33.314335Z","iopub.status.idle":"2025-05-02T07:14:40.384863Z","shell.execute_reply.started":"2025-05-02T07:14:33.314309Z","shell.execute_reply":"2025-05-02T07:14:40.383991Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m811.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:14:44.633077Z","iopub.execute_input":"2025-05-02T07:14:44.633327Z","iopub.status.idle":"2025-05-02T07:14:44.637238Z","shell.execute_reply.started":"2025-05-02T07:14:44.633308Z","shell.execute_reply":"2025-05-02T07:14:44.636529Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer, TrainerCallback\nfrom datasets import Dataset\nfrom seqeval.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport ast\nimport torch\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:18:43.452160Z","iopub.execute_input":"2025-05-02T07:18:43.452845Z","iopub.status.idle":"2025-05-02T07:18:43.458995Z","shell.execute_reply.started":"2025-05-02T07:18:43.452821Z","shell.execute_reply":"2025-05-02T07:18:43.458324Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load the CSV files assuming keys \"train.csv\" and \"test.csv\" in uploaded dict\ntrain_df = pd.read_csv(\"/kaggle/input/comp4211/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/comp4211/test.csv\")\n\n# Parse the stringified lists (adjust column names if different)\ntrain_df['Sentence'] = train_df['Sentence'].apply(ast.literal_eval)\ntrain_df['NER Tag'] = train_df['NER Tag'].apply(ast.literal_eval)\ntest_df['Sentence'] = test_df['Sentence'].apply(ast.literal_eval)\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:18:45.962526Z","iopub.execute_input":"2025-05-02T07:18:45.963498Z","iopub.status.idle":"2025-05-02T07:18:49.356537Z","shell.execute_reply.started":"2025-05-02T07:18:45.963462Z","shell.execute_reply":"2025-05-02T07:18:49.355933Z"}},"outputs":[{"name":"stdout","text":"Train shape: (40000, 3)\nTest shape: (5000, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Split into train/validation (adjust test_size as preferred)\nfull_df = train_df # saving the full dataset for further training\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\n# sets used for hyperparameter tuning (20% of training set and validation set)\n# train_tuning, remaining_test = train_test_split(train_df, test_size=0.8, random_state=42)\n# val_tuning, remaining_validation = train_test_split(val_df, test_size=0.8, random_state=42)\n\n# Get unique labels from training set and create mapping dictionaries.\nunique_labels = sorted({label for tags in train_df['NER Tag'] for label in tags})\nlabel2id = {label: i for i, label in enumerate(unique_labels)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(\"Unique labels:\", unique_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:18:54.110444Z","iopub.execute_input":"2025-05-02T07:18:54.111323Z","iopub.status.idle":"2025-05-02T07:18:54.167181Z","shell.execute_reply.started":"2025-05-02T07:18:54.111296Z","shell.execute_reply":"2025-05-02T07:18:54.166609Z"}},"outputs":[{"name":"stdout","text":"Unique labels: ['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-cased\")\n\ndef encode_examples(example):\n    # Tokenize input (word-level input, not sentence string)\n    tokenized_input = tokenizer(\n        example[\"Sentence\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        is_split_into_words=True,\n        return_tensors=\"pt\"\n    )\n\n    word_ids = tokenized_input.word_ids(batch_index=0)  # for single example\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n        if word_idx is None:\n            label_ids.append(-100)\n        elif word_idx != previous_word_idx:\n            label_ids.append(label2id[example[\"NER Tag\"][word_idx]])\n        else:\n            label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    tokenized_input[\"labels\"] = torch.tensor(label_ids)\n\n    # Remove batch dimension for Hugging Face datasets map compatibility\n    return {k: v.squeeze() if isinstance(v, torch.Tensor) else v for k, v in tokenized_input.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:18:58.052335Z","iopub.execute_input":"2025-05-02T07:18:58.053158Z","iopub.status.idle":"2025-05-02T07:19:00.238047Z","shell.execute_reply.started":"2025-05-02T07:18:58.053115Z","shell.execute_reply":"2025-05-02T07:19:00.237311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c539bfb36d984be09768995d74824355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a1c086e869447e86b7ec9dfc9ae1d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0f56e9c6c8450498f97316e15903bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604508b5aaf14dd18cab78ff14f7af1c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Convert dataframes to datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Apply tokenization function\ntrain_dataset = train_dataset.map(encode_examples, remove_columns=train_dataset.column_names)\nval_dataset = val_dataset.map(encode_examples, remove_columns=val_dataset.column_names)\n\n# train_tuning = Dataset.from_pandas(train_tuning)\n# val_tuning = Dataset.from_pandas(val_tuning)\n# train_tuning = train_tuning.map(encode_examples, remove_columns = train_tuning.column_names)\n# val_tuning = val_tuning.map(encode_examples, remove_columns = val_tuning.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:19:13.717330Z","iopub.execute_input":"2025-05-02T07:19:13.717637Z","iopub.status.idle":"2025-05-02T07:19:39.129830Z","shell.execute_reply.started":"2025-05-02T07:19:13.717613Z","shell.execute_reply":"2025-05-02T07:19:39.128954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5e0f58f7594e65b345211cb2b7e26e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0414b14f129e4a789b9cddbb63a42aab"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# custom checkpoint callback\nclass SaveCheckpointCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n        out = '/kaggle/working/checkpoints'\n        os.makedirs(out, exist_ok=True)\n        model.save_pretrained(f\"{out}/epoch_{int(state.epoch)}\")\n        tokenizer.save_pretrained(f\"{out}/epoch_{int(state.epoch)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:19:43.276762Z","iopub.execute_input":"2025-05-02T07:19:43.277013Z","iopub.status.idle":"2025-05-02T07:19:43.281279Z","shell.execute_reply.started":"2025-05-02T07:19:43.276996Z","shell.execute_reply":"2025-05-02T07:19:43.280624Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Set up the model\nmodel = BertForTokenClassification.from_pretrained(\n    \"bert-large-cased\",\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:19:46.987244Z","iopub.execute_input":"2025-05-02T07:19:46.987945Z","iopub.status.idle":"2025-05-02T07:19:53.285314Z","shell.execute_reply.started":"2025-05-02T07:19:46.987915Z","shell.execute_reply":"2025-05-02T07:19:53.284609Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8d341929a349789976b867a93c8b4f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Optional: Send model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:19:55.377343Z","iopub.execute_input":"2025-05-02T07:19:55.378125Z","iopub.status.idle":"2025-05-02T07:19:55.978555Z","shell.execute_reply.started":"2025-05-02T07:19:55.378089Z","shell.execute_reply":"2025-05-02T07:19:55.977821Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Define metric computation\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [\n        [id2label[label] for label in sent_labels if label != -100]\n        for sent_labels in labels\n    ]\n    true_predictions = [\n        [id2label[p] for (p, l) in zip(sent_preds, sent_labels) if l != -100]\n        for sent_preds, sent_labels in zip(predictions, labels)\n    ]\n\n    # Using seqeval's f1 score\n    return {\"f1\": f1_score(true_labels, true_predictions)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:19:58.049669Z","iopub.execute_input":"2025-05-02T07:19:58.050200Z","iopub.status.idle":"2025-05-02T07:19:58.054591Z","shell.execute_reply.started":"2025-05-02T07:19:58.050178Z","shell.execute_reply":"2025-05-02T07:19:58.053888Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Grid search:\n# param_grid = {\n#     'learning_rate': [2e-5, 6e-5, 1e-4],\n#     'per_device_train_batch_size': [8, 16],\n#     'num_train_epochs': [2],\n#     'weight_decay': [0.01]\n# }\n\n# best_f1 = 0\n# best_params = {}\n\n# for params in ParameterGrid(param_grid):\n#     print(f\"Training with parameters: {params}\")\n#     model_dir = f\"model_lr_{params['learning_rate']}_bs_{params['per_device_train_batch_size']}_epochs_{params['num_train_epochs']}_wd_{params['weight_decay']}\"\n#     training_args = TrainingArguments(\n#         output_dir=\"./results\",\n#         eval_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         learning_rate=params['learning_rate'],\n#         per_device_train_batch_size=params['per_device_train_batch_size'],\n#         per_device_eval_batch_size=16, #evaluation batch size consistent\n#         num_train_epochs=params['num_train_epochs'],\n#         weight_decay=params['weight_decay'],\n#         save_total_limit=1,\n#         push_to_hub=False,\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"f1\"  #use f1 as metric\n#     )\n\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_tuning,\n#         eval_dataset=val_tuning,\n#         compute_metrics=compute_metrics,\n#         tokenizer=tokenizer\n#     )\n\n#     trainer.train()\n\n#     eval_results = trainer.evaluate()\n#     f1 = eval_results.get(\"eval_f1\")\n#     print(f1)\n#     trainer.save_model(model_dir)\n\n#     if f1 > best_f1:\n#         best_f1 = f1\n#         best_params = params\n#         print(f\"New best F1: {best_f1} with parameters: {best_params}\")\n\n#         #Saving the best model\n#         trainer.save_model(f\"best_model_f1_{best_f1:.4f}\")\n\n# print(f\"Best hyperparameters: {best_params}\")\n# print(f\"Best F1 score: {best_f1}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=1,\n    push_to_hub=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[SaveCheckpointCallback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T07:20:00.443811Z","iopub.execute_input":"2025-05-02T07:20:00.444132Z","iopub.status.idle":"2025-05-02T07:20:00.751433Z","shell.execute_reply.started":"2025-05-02T07:20:00.444100Z","shell.execute_reply":"2025-05-02T07:20:00.750723Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_31/3991841624.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Train!\ntrainer.train()\n\n# Save the model to a local directory (e.g., 'model')\ntrainer.save_model(\"/kaggle/working/bert_large_model\")\n\n# zip checkpoints\nshutil.make_archive('/kaggle/working/checkpoints', 'zip', '/kaggle/working/checkpoints')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T08:21:49.095391Z","iopub.execute_input":"2025-05-02T08:21:49.095937Z","iopub.status.idle":"2025-05-02T09:47:54.860591Z","shell.execute_reply.started":"2025-05-02T08:21:49.095913Z","shell.execute_reply":"2025-05-02T09:47:54.859830Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6750/6750 1:22:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.047700</td>\n      <td>0.093634</td>\n      <td>0.839624</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.033100</td>\n      <td>0.113291</td>\n      <td>0.838654</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.016000</td>\n      <td>0.129469</td>\n      <td>0.842940</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/checkpoints.zip'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"param_count = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {param_count:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T09:47:55.065550Z","iopub.execute_input":"2025-05-02T09:47:55.066305Z","iopub.status.idle":"2025-05-02T09:47:55.073182Z","shell.execute_reply.started":"2025-05-02T09:47:55.066274Z","shell.execute_reply":"2025-05-02T09:47:55.072159Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 332,547,089\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from safetensors.torch import save_file\n\nsave_file(model.state_dict(), \"bert_large.safetensors\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T09:48:36.558512Z","iopub.execute_input":"2025-05-02T09:48:36.559057Z","iopub.status.idle":"2025-05-02T09:48:39.614398Z","shell.execute_reply.started":"2025-05-02T09:48:36.559036Z","shell.execute_reply":"2025-05-02T09:48:39.613837Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'bert_large.safetensors')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T09:49:02.027532Z","iopub.execute_input":"2025-05-02T09:49:02.027827Z","iopub.status.idle":"2025-05-02T09:49:02.032844Z","shell.execute_reply.started":"2025-05-02T09:49:02.027808Z","shell.execute_reply":"2025-05-02T09:49:02.032181Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/bert_large.safetensors","text/html":"<a href='bert_large.safetensors' target='_blank'>bert_large.safetensors</a><br>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# # loading weights to the local machine\n# from safetensors.torch import load_file\n# import torch\n# from transformers import BertForSequenceClassification\n\n# model = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n# state_dict = load_file('bert_large_finetuned.safetensors')\n# model.load_state_dict(state_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a tokenization-only function for test set\ndef tokenize_test(example):\n    tokenized = tokenizer(\n        example[\"Sentence\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        is_split_into_words=True,\n        return_tensors=None  # leave as lists for HF Dataset\n    )\n    tokenized[\"word_ids\"] = tokenized.word_ids()  # store the word_ids\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T09:50:02.087739Z","iopub.execute_input":"2025-05-02T09:50:02.088028Z","iopub.status.idle":"2025-05-02T09:50:02.092608Z","shell.execute_reply.started":"2025-05-02T09:50:02.088008Z","shell.execute_reply":"2025-05-02T09:50:02.091880Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# 1) Check for duplicate IDs\ndupes = test_df[\"id\"][test_df[\"id\"].duplicated()]\nif len(dupes):\n    print(f\"Duplicate IDs found: {dupes.tolist()}\")\nelse:\n    print(\"No duplicate IDs.\")\n\n# 2) Check for sentences that get truncated by the tokenizer\ntoo_long = []\nfor idx, sent in enumerate(test_df[\"Sentence\"]):\n    toks = tokenizer(\n        sent,\n        truncation=True,\n        padding=False,\n        max_length=128,\n        is_split_into_words=True\n    )\n    # count actual word_ids (excluding special tokens)\n    wids = toks.word_ids()\n    # words retained = max word_idx + 1\n    max_word = max([w for w in wids if w is not None], default=-1) + 1\n    if max_word < len(sent):\n        too_long.append((test_df[\"id\"].iloc[idx], len(sent), max_word))\n\nif too_long:\n    print(\"Sentences being truncated (id, orig_len, kept_words):\")\n    for t in too_long[:5]:\n        print(\" \", t)\nelse:\n    print(\"No truncation issues (all sentences ≤128 tokens).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:27:23.385920Z","iopub.execute_input":"2025-05-02T10:27:23.386487Z","iopub.status.idle":"2025-05-02T10:27:24.161735Z","shell.execute_reply.started":"2025-05-02T10:27:23.386466Z","shell.execute_reply":"2025-05-02T10:27:24.161089Z"}},"outputs":[{"name":"stdout","text":"No duplicate IDs.\nNo truncation issues (all sentences ≤128 tokens).\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# 1) Tokenize test set once, storing word_ids and keeping Sentence\ndef tokenize_with_word_ids(example):\n    # Keep the original sentence for later length check\n    sent = example[\"Sentence\"]\n    tokenized = tokenizer(\n        sent,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        is_split_into_words=True\n    )\n    example[\"word_ids\"] = tokenized.word_ids()\n    # store tokenized fields\n    example.update(tokenized)\n    return example\n\n# Build the test dataset, but don't drop 'Sentence' yet\ntest_dataset = Dataset.from_pandas(test_df)\ntest_dataset = test_dataset.map(\n    tokenize_with_word_ids,\n    remove_columns=[]  # keep all columns, including 'Sentence'\n)\n\n# 2) Align predictions: but first, prepare inputs only\n# Create a view with only the model inputs\npred_dataset = test_dataset.remove_columns([\"Sentence\", \"id\"])  # keep only input_ids, attention_mask, token_type_ids, word_ids\npred_dataset.set_format(type=\"torch\")\n\n# 3) Run predictions\nraw_preds = trainer.predict(pred_dataset)\npreds = np.argmax(raw_preds.predictions, axis=2)\n\n# 4) Align predictions to original words, padding if needed\nfinal_preds = []\nfor sent, pred_row, word_ids in zip(test_dataset[\"Sentence\"], preds, test_dataset[\"word_ids\"]):\n    aligned = []\n    prev = None\n    for idx, widx in enumerate(word_ids):\n        if widx is not None and widx != prev:\n            aligned.append(id2label[pred_row[idx]])\n        prev = widx\n\n    # If for some reason we have fewer tags than words, pad with 'O'\n    if len(aligned) < len(sent):\n        padding = [\"O\"] * (len(sent) - len(aligned))\n        aligned.extend(padding)\n\n    # Or if too many (shouldn't happen), truncate\n    if len(aligned) > len(sent):\n        aligned = aligned[: len(sent)]\n\n    final_preds.append(aligned)\n\n# 5) Sanity-check lengths\nmismatches = [\n    (ex[\"id\"], len(ex[\"Sentence\"]), len(p))\n    for ex, p in zip(test_dataset, final_preds)\n    if len(ex[\"Sentence\"]) != len(p)\n]\nif mismatches:\n    print(\"Still mismatches (should be none):\", mismatches)\nelse:\n    print(\"All lengths match after padding!\")\n\n# 6) Build submission\nsubmission_df = pd.DataFrame({\n    \"id\": test_dataset[\"id\"],\n    \"NER Tag\": [str(p) for p in final_preds]\n})\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file generated!\")\n# files.download(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:39:34.605122Z","iopub.execute_input":"2025-05-02T10:39:34.605615Z","iopub.status.idle":"2025-05-02T10:40:43.184708Z","shell.execute_reply.started":"2025-05-02T10:39:34.605589Z","shell.execute_reply":"2025-05-02T10:40:43.184068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a90e9aa26f1413b81ae4d209da2f1fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"All lengths match after padding!\nSubmission file generated!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:47:12.789314Z","iopub.execute_input":"2025-05-02T11:47:12.790133Z","iopub.status.idle":"2025-05-02T11:47:12.815088Z","shell.execute_reply.started":"2025-05-02T11:47:12.790110Z","shell.execute_reply":"2025-05-02T11:47:12.814521Z"}},"outputs":[],"execution_count":42}]}